#total layer/optimizer
100 sgd
#batch size
1000
#input chanal x heigth x weigth x trainSample x testSample
3 32 32 50000 10000
#output chanal x heigth x weigth 
10
#input Source File
data/cifar10train.bin
#output Source File
data/cifar10test.bin
#learningRate momentum isTraning learningDecay
0.010000 0.950000 1 0.000000
###net ConnectParameter####
####
#connectMorph=c inputLayerNumber=0 outputLayerNumber=1 ConnectType=conv2d 
#feature=64 convolutionKernelHeight=7 convolutionKernelWidth=7 
#convStride=2 convPadding=3 convDilation=1
c 0 1 conv2d 64 7 7 2 3 1
#################################
#connectMorph=c inputLayerNumber=1 outputLayerNumber=2 ConnectType=batchNormCha 
#BatchNormalizationMomentum=0.999 BatchNormalizationEpsilon=0.000011
c 1 2 batchNormCha 0.999000 0.000011
#################################
#connectMorph=c inputLayerNumber=2 outputLayerNumber=3
# ConnectType=relu(sigm-tanh-crelu-elu)
#ActivationAlpha=1  
c 2 3 relu 1
###################################
#connectMorph=c inputLayerNumber=3 outputLayerNumber=4 ConnectType=poolmax (poolmax-poolavg)
#poolKernelSize=2  poolStride=2 poolPadding=0 
c 3 4 poolmax 3 2 1
c 4 5 conv2d 64 3 3 1 1 1
c 5 6 batchNormCha 0.999000 0.000011
c 6 7 relu 1
c 7 8 conv2d 64 3 3 1 1 1
c 8 9 batchNormCha 0.999000 0.000011
###################################
#connectMorph=c inputLayerNumber=4 outputLayerNumber=9 ConnectType=add 
#poolKernelSize=2  poolStride=2 poolPadding=0 
c 4 9 add
c 9 10 relu
c 10 11 conv2d 64 3 3 1 1 1
c 11 12 batchNormCha 0.999000 0.000011
c 12 13 relu
c 13 14 conv2d 64 3 3 1 1 1
c 14 15 batchNormCha 0.999000 0.000011
c 10 15 add
c 15 16 relu
c 16 17 conv2d 128 3 3 2 1 1
c 17 18 batchNormCha 0.999000 0.000011
c 18 19 relu
c 19 20 conv2d 128 3 3 1 1 1
c 20 21 batchNormCha 0.999000 0.000011
c 21 21 add
c 21 22 relu
c 22 23 conv2d 128 3 3 1 1 1
c 23 24 batchNormCha 0.999000 0.000011
c 24 25 relu
c 25 26 conv2d 128 3 3 1 1 1
c 26 27 batchNormCha 0.999000 0.000011
c 22 27 add
c 27 28 relu
c 28 29 conv2d 256 3 3 2 1 1
c 29 30 batchNormCha 0.999000 0.000011
c 30 31 relu
c 31 32 conv2d 256 3 3 1 1 1
c 32 33 batchNormCha 0.999000 0.000011
c 33 33 add
c 33 34 relu
c 34 35 conv2d 256 3 3 1 1 1
c 35 36 batchNormCha 0.999000 0.000011
c 36 37 relu
c 37 38 conv2d 256 3 3 1 1 1
c 38 39 batchNormCha 0.999000 0.000011
c 34 39 add
c 39 40 relu
c 40 41 conv2d 512 3 3 2 1 1
c 41 42 batchNormCha 0.999000 0.000011
c 42 43 relu
c 43 44 conv2d 512 3 3 1 1 1
c 44 45 batchNormCha 0.999000 0.000011
c 45 45 add
c 45 46 relu
c 46 47 conv2d 512 3 3 1 1 1
c 47 48 batchNormCha 0.999000 0.000011
c 48 49 relu
c 49 50 conv2d 512 3 3 1 1 1
c 50 51 batchNormCha 0.999000 0.000011
c 46 51 add
c 51 52 relu
###################################
#connectMorph=c inputLayerNumber=52 outputLayerNumber=53 ConnectType=dropout 
#dropoutThreshold=0.500
c 52 53 dropout 0.500000
###################################
#connectMorph=c inputLayerNumber=53 outputLayerNumber=54 ConnectType=fully 
#fullyConnectHiddenNeuron=500
c 53 54 fully 500
c 54 55 relu
c 55 56 fully 10
c 56 57 softmax